# -*- coding: utf-8 -*-
"""
Streamlitåº”ç”¨ä¸»æ–‡ä»¶ - ç›¸ä¼¼é¢˜ç›®åŒ¹é…ç³»ç»Ÿ
"""

import streamlit as st
import pandas as pd
import numpy as np
import asyncio
import pickle
import os
from datetime import datetime
from typing import Dict, List, Tuple, Any
import io

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from embedding_service import EmbeddingService
from similarity_matcher import SimilarityMatcher
from llm_service import LLMService
from config import (
    EMBEDDINGS_CACHE_PATH, TOP_K_SIMILAR, SIMILARITY_THRESHOLD, 
    VERBOSE
)

# å®šä¹‰ç›¸ä¼¼åº¦æ–¹æ³•å’Œè¾“å‡ºåˆ—é…ç½®
SIMILARITY_METHOD = "cosine"
OUTPUT_COLUMNS = {
    'query': 'é¢˜ç›®',
    'similar_text_1': 'ç›¸ä¼¼é¢˜ç›®1',
    'similarity_score_1': 'ç›¸ä¼¼åº¦1',
    'similar_text_2': 'ç›¸ä¼¼é¢˜ç›®2',
    'similarity_score_2': 'ç›¸ä¼¼åº¦2',
    'similar_text_3': 'ç›¸ä¼¼é¢˜ç›®3',
    'similarity_score_3': 'ç›¸ä¼¼åº¦3'
}


class SimilarityMatchingApp:
    """ç›¸ä¼¼é¢˜ç›®åŒ¹é…åº”ç”¨ç±»"""
    
    def __init__(self):
        self.embedding_service = EmbeddingService()
        self.similarity_matcher = SimilarityMatcher()
        self.llm_service = LLMService()
        self.base_embeddings = None
        self.base_texts = None
        
    def load_base_embeddings(self) -> bool:
        """åŠ è½½åŸºç¡€é¢˜åº“çš„å‘é‡æ•°æ®"""
        try:
            if os.path.exists(EMBEDDINGS_CACHE_PATH):
                with open(EMBEDDINGS_CACHE_PATH, 'rb') as f:
                    cache_data = pickle.load(f)
                
                # å…¼å®¹å¤šç§ç¼“å­˜æ ¼å¼
                if isinstance(cache_data, dict):
                    # ç›®æ ‡æ ¼å¼: { 'embeddings': Dict[str, np.ndarray], 'texts': List[str] }
                    if 'embeddings' in cache_data and 'texts' in cache_data:
                        self.base_embeddings = cache_data['embeddings']
                        self.base_texts = cache_data['texts']
                        st.success(f"âœ… æˆåŠŸåŠ è½½åŸºç¡€é¢˜åº“ï¼Œå…± {len(self.base_texts)} æ¡æ•°æ®")
                        return True
                    
                    # å…¼å®¹æ ¼å¼: { 'embeddings': { id: { 'text': str, 'embedding': np.ndarray } } }
                    elif 'embeddings' in cache_data and isinstance(cache_data['embeddings'], dict):
                        raw = cache_data['embeddings']
                        reconstructed: Dict[str, np.ndarray] = {}
                        texts: List[str] = []
                        for _id, item in raw.items():
                            try:
                                # æƒ…å†µAï¼šitemä¸ºåŒ…å«text/embeddingçš„å­—å…¸
                                if isinstance(item, dict):
                                    text = item.get('text')
                                    emb = item.get('embedding')
                                    if text is None:
                                        # å…¼å®¹å¯èƒ½çš„é”®å
                                        text = item.get('content') or item.get('title') or item.get('question')
                                    if text is not None and emb is not None:
                                        if not isinstance(emb, np.ndarray):
                                            emb = np.array(emb, dtype=np.float32)
                                        reconstructed[text] = emb
                                        texts.append(text)
                                else:
                                    # æƒ…å†µBï¼šç›´æ¥æ˜ å°„ä¸º { æ–‡æœ¬: å‘é‡ }
                                    text = str(_id)
                                    emb = item
                                    if emb is not None:
                                        if not isinstance(emb, np.ndarray):
                                            emb = np.array(emb, dtype=np.float32)
                                        reconstructed[text] = emb
                                        texts.append(text)
                            except Exception:
                                continue
                        if reconstructed:
                            self.base_embeddings = reconstructed
                            self.base_texts = texts
                            st.success(f"âœ… æˆåŠŸåŠ è½½åŸºç¡€é¢˜åº“ï¼Œå…± {len(self.base_texts)} æ¡æ•°æ®")
                            return True
                        else:
                            st.error("âŒ ç¼“å­˜æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼ˆæœªè§£æåˆ°æœ‰æ•ˆçš„æ–‡æœ¬ä¸å‘é‡ï¼‰")
                            return False
                    else:
                        st.error("âŒ ç¼“å­˜æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
                        return False
                else:
                    st.error("âŒ ç¼“å­˜æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
                    return False
            else:
                st.error(f"âŒ æœªæ‰¾åˆ°åŸºç¡€é¢˜åº“å‘é‡æ–‡ä»¶: {EMBEDDINGS_CACHE_PATH}")
                return False
        except Exception as e:
            st.error(f"âŒ åŠ è½½åŸºç¡€é¢˜åº“å‘é‡æ•°æ®å¤±è´¥: {str(e)}")
            return False
    
    async def process_user_texts(self, user_texts: List[str]) -> List[np.ndarray]:
        """å¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ–‡æœ¬ï¼Œç”Ÿæˆå‘é‡"""
        try:
            # å¼‚æ­¥è¯·æ±‚Embedding
            embedding_results = await self.embedding_service.get_embeddings_batch(user_texts)
            # è§£ç ä¸ºnumpyå‘é‡ï¼Œå¹¶æŒ‰åŸå§‹é¡ºåºå¯¹é½
            processed = self.embedding_service.process_embeddings(embedding_results)
            processed.sort(key=lambda x: x.get('index', 0))
            embeddings = [item['embedding'] for item in processed]
            return embeddings
        except Exception as e:
            st.error(f"å‘é‡åŒ–å¤„ç†å¤±è´¥: {str(e)}")
            return []
    
    def find_similar_texts(self, user_embeddings: List[np.ndarray], user_texts: List[str]) -> Dict[str, List[Tuple[str, float]]]:
        """æŸ¥æ‰¾ç›¸ä¼¼æ–‡æœ¬"""
        similarity_results = {}
        
        for i, (user_embedding, user_text) in enumerate(zip(user_embeddings, user_texts)):
            try:
                # ä½¿ç”¨ç›¸ä¼¼åº¦åŒ¹é…å™¨æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„æ–‡æœ¬
                similar_items = self.similarity_matcher.find_most_similar(
                    query_embedding=user_embedding,
                    candidate_embeddings=self.base_embeddings,
                    similarity_method=SIMILARITY_METHOD
                )
                
                similarity_results[user_text] = similar_items
                
            except Exception as e:
                st.error(f"å¤„ç†æ–‡æœ¬ '{user_text[:50]}...' æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                similarity_results[user_text] = []
        
        return similarity_results
    
    async def enhance_with_llm(self, similarity_results: Dict[str, List[Tuple[str, float]]]) -> Dict[str, List[Tuple[str, float, str]]]:
        """ä½¿ç”¨LLMå¢å¼ºç›¸ä¼¼åº¦ç»“æœ"""
        try:
            enhanced_results = await self.llm_service.enhance_similarity_results(similarity_results)
            return enhanced_results
        except Exception as e:
            st.error(f"LLMå¢å¼ºå¤„ç†å¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤å¢å¼ºç»“æœ
            enhanced_results = {}
            for query_text, candidates in similarity_results.items():
                enhanced_results[query_text] = [
                    (text, score, "LLMå¤„ç†å¤±è´¥") for text, score in candidates
                ]
            return enhanced_results
    
    def create_output_dataframe(self, enhanced_results: Dict[str, List[Tuple[str, float, str]]]) -> pd.DataFrame:
        """åˆ›å»ºè¾“å‡ºDataFrame - æ°´å¹³æ’åˆ—æ ¼å¼"""
        output_data = []
        
        for query_text, results in enhanced_results.items():
            # åˆ›å»ºä¸€è¡Œæ•°æ®ï¼ŒåŒ…å«æŸ¥è¯¢é¢˜ç›®å’Œæœ€å¤š3ä¸ªç›¸ä¼¼é¢˜ç›®
            row_data = {OUTPUT_COLUMNS['query']: query_text}
            
            # å¡«å……ç›¸ä¼¼é¢˜ç›®å’Œç›¸ä¼¼åº¦ï¼ˆæœ€å¤š3ä¸ªï¼‰
            for i in range(3):
                similar_key = f'similar_text_{i+1}'
                score_key = f'similarity_score_{i+1}'
                
                if i < len(results):
                    similar_text, similarity_score, _ = results[i]
                    row_data[OUTPUT_COLUMNS[similar_key]] = similar_text
                    row_data[OUTPUT_COLUMNS[score_key]] = round(similarity_score, 4)
                else:
                    # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„ç›¸ä¼¼é¢˜ç›®ï¼Œå¡«å……ç©ºå€¼
                    row_data[OUTPUT_COLUMNS[similar_key]] = ""
                    row_data[OUTPUT_COLUMNS[score_key]] = ""
            
            output_data.append(row_data)
        
        return pd.DataFrame(output_data)
    
    def create_excel_download(self, df: pd.DataFrame) -> bytes:
        """åˆ›å»ºExcelæ–‡ä»¶ç”¨äºä¸‹è½½"""
        output = io.BytesIO()
        
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='ç›¸ä¼¼é¢˜ç›®åŒ¹é…ç»“æœ')
            
            # è·å–å·¥ä½œè¡¨å¹¶è°ƒæ•´åˆ—å®½
            worksheet = writer.sheets['ç›¸ä¼¼é¢˜ç›®åŒ¹é…ç»“æœ']
            for column in worksheet.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)  # é™åˆ¶æœ€å¤§å®½åº¦
                worksheet.column_dimensions[column_letter].width = adjusted_width
        
        output.seek(0)
        return output.getvalue()


def main():
    """ä¸»å‡½æ•°"""
    st.set_page_config(
        page_title="ç›¸ä¼¼é¢˜ç›®åŒ¹é…ç³»ç»Ÿ",
        page_icon="ğŸ”",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸ” ç›¸ä¼¼é¢˜ç›®åŒ¹é…ç³»ç»Ÿ")
    st.markdown("---")
    
    # åˆå§‹åŒ–åº”ç”¨
    if 'app' not in st.session_state:
        st.session_state.app = SimilarityMatchingApp()
    
    app = st.session_state.app
    
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ ç³»ç»Ÿé…ç½®")
        
        # æ˜¾ç¤ºå½“å‰é…ç½®
        st.subheader("å½“å‰é…ç½®")
        st.write(f"**ç›¸ä¼¼åº¦æ–¹æ³•**: {SIMILARITY_METHOD}")
        st.write(f"**TOP-K**: {TOP_K_SIMILAR}")
        st.write(f"**ç›¸ä¼¼åº¦é˜ˆå€¼**: {SIMILARITY_THRESHOLD}")
        
        st.markdown("---")
        
        # ç³»ç»ŸçŠ¶æ€
        st.subheader("ğŸ“Š ç³»ç»ŸçŠ¶æ€")
        
        # æ£€æŸ¥åŸºç¡€é¢˜åº“çŠ¶æ€
        if app.load_base_embeddings():
            st.success(f"âœ… åŸºç¡€é¢˜åº“å·²åŠ è½½ ({len(app.base_texts)} æ¡)")
            base_loaded = True
        else:
            st.error("âŒ åŸºç¡€é¢˜åº“åŠ è½½å¤±è´¥")
            base_loaded = False
    
    # ä¸»ç•Œé¢
    if not base_loaded:
        st.error("âš ï¸ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥åŸºç¡€é¢˜åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        st.info(f"è¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨: {EMBEDDINGS_CACHE_PATH}")
        return
    
    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    st.header("ğŸ“ ä¸Šä¼ æ–°é¢˜åº“")
    
    uploaded_file = st.file_uploader(
        "é€‰æ‹©CSVæ–‡ä»¶ (UTF-8 BOMç¼–ç ï¼Œç¬¬ä¸€åˆ—ä¸ºé¢˜ç›®æ–‡æœ¬)",
        type=['csv'],
        help="è¯·ä¸Šä¼ UTF-8 BOMç¼–ç çš„CSVæ–‡ä»¶ï¼Œç¬¬ä¸€åˆ—åº”åŒ…å«éœ€è¦åŒ¹é…çš„é¢˜ç›®æ–‡æœ¬"
    )
    
    if uploaded_file is not None:
        try:
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(uploaded_file, encoding='utf-8-sig')
            
            if df.empty:
                st.error("ä¸Šä¼ çš„æ–‡ä»¶ä¸ºç©º")
                return
            
            # è·å–ç¬¬ä¸€åˆ—æ•°æ®
            first_column = df.iloc[:, 0]
            user_texts = first_column.dropna().astype(str).tolist()
            
            if not user_texts:
                st.error("ç¬¬ä¸€åˆ—æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬æ•°æ®")
                return
            
            st.success(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œå…±è¯»å–åˆ° {len(user_texts)} æ¡é¢˜ç›®")
            
            # æ˜¾ç¤ºé¢„è§ˆ
            with st.expander("ğŸ“‹ æ•°æ®é¢„è§ˆ", expanded=False):
                preview_df = pd.DataFrame({
                    'åºå·': range(1, min(11, len(user_texts) + 1)),
                    'é¢˜ç›®æ–‡æœ¬': user_texts[:10]
                })
                st.dataframe(preview_df, use_container_width=True)
                
                if len(user_texts) > 10:
                    st.info(f"ä»…æ˜¾ç¤ºå‰10æ¡ï¼Œæ€»å…±{len(user_texts)}æ¡")
            
            # å¤„ç†æŒ‰é’®
            col1, col2, col3 = st.columns([1, 2, 1])
            
            with col2:
                if st.button("ğŸš€ å¼€å§‹åŒ¹é…", type="primary", use_container_width=True):
                    # åˆ›å»ºè¿›åº¦æ¡å’ŒçŠ¶æ€æ˜¾ç¤º
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    try:
                        # æ­¥éª¤1: å‘é‡åŒ–ç”¨æˆ·æ–‡æœ¬
                        status_text.text("ğŸ”„ æ­£åœ¨å‘é‡åŒ–ç”¨æˆ·æ–‡æœ¬...")
                        progress_bar.progress(20)
                        
                        # ä½¿ç”¨asyncioè¿è¡Œå¼‚æ­¥å‡½æ•°
                        user_embeddings = asyncio.run(app.process_user_texts(user_texts))
                        
                        if not user_embeddings:
                            st.error("å‘é‡åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®")
                            return
                        
                        # æ­¥éª¤2: æŸ¥æ‰¾ç›¸ä¼¼æ–‡æœ¬
                        status_text.text("ğŸ” æ­£åœ¨æŸ¥æ‰¾ç›¸ä¼¼æ–‡æœ¬...")
                        progress_bar.progress(40)
                        
                        similarity_results = app.find_similar_texts(user_embeddings, user_texts)
                        
                        # æ­¥éª¤3: LLMå¢å¼º
                        status_text.text("ğŸ¤– æ­£åœ¨ä½¿ç”¨LLMå¢å¼ºç»“æœ...")
                        progress_bar.progress(60)
                        
                        enhanced_results = asyncio.run(app.enhance_with_llm(similarity_results))
                        
                        # æ­¥éª¤4: ç”Ÿæˆç»“æœ
                        status_text.text("ğŸ“Š æ­£åœ¨ç”Ÿæˆç»“æœ...")
                        progress_bar.progress(80)
                        
                        result_df = app.create_output_dataframe(enhanced_results)
                        
                        # æ­¥éª¤5: å®Œæˆ
                        status_text.text("âœ… å¤„ç†å®Œæˆï¼")
                        progress_bar.progress(100)
                        
                        # æ˜¾ç¤ºç»“æœ
                        st.markdown("---")
                        st.header("ğŸ“‹ åŒ¹é…ç»“æœ")
                        
                        # ç»“æœç»Ÿè®¡
                        col1, col2, col3, col4 = st.columns(4)
                        
                        with col1:
                            st.metric("æŸ¥è¯¢é¢˜ç›®æ•°", len(user_texts))
                        
                        with col2:
                            # è®¡ç®—æ€»çš„åŒ¹é…ç»“æœæ•°ï¼ˆéç©ºçš„ç›¸ä¼¼é¢˜ç›®ï¼‰
                            total_matches = 0
                            for i in range(1, 4):
                                col_name = OUTPUT_COLUMNS[f'similar_text_{i}']
                                total_matches += len(result_df[result_df[col_name] != ""])
                            st.metric("åŒ¹é…ç»“æœæ•°", total_matches)
                        
                        with col3:
                            # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦ï¼ˆæ‰€æœ‰éç©ºç›¸ä¼¼åº¦çš„å¹³å‡å€¼ï¼‰
                            all_scores = []
                            for i in range(1, 4):
                                score_col = OUTPUT_COLUMNS[f'similarity_score_{i}']
                                scores = result_df[result_df[score_col] != ""][score_col]
                                all_scores.extend(scores.tolist())
                            avg_score = sum(all_scores) / len(all_scores) if all_scores else 0
                            st.metric("å¹³å‡ç›¸ä¼¼åº¦", f"{avg_score:.3f}")
                        
                        with col4:
                            # è®¡ç®—é«˜è´¨é‡åŒ¹é…æ•°ï¼ˆç›¸ä¼¼åº¦>=0.7çš„åŒ¹é…ï¼‰
                            high_quality_matches = 0
                            for i in range(1, 4):
                                score_col = OUTPUT_COLUMNS[f'similarity_score_{i}']
                                high_quality_matches += len(result_df[(result_df[score_col] != "") & (result_df[score_col] >= 0.7)])
                            st.metric("é«˜è´¨é‡åŒ¹é…", f"{high_quality_matches}")
                        
                        # æ˜¾ç¤ºç»“æœè¡¨æ ¼
                        st.subheader("è¯¦ç»†ç»“æœ")
                        st.dataframe(result_df, use_container_width=True)
                        
                        # ä¸‹è½½æŒ‰é’®
                        excel_data = app.create_excel_download(result_df)
                        
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename = f"ç›¸ä¼¼é¢˜ç›®åŒ¹é…ç»“æœ_{timestamp}.xlsx"
                        
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è½½Excelæ–‡ä»¶",
                            data=excel_data,
                            file_name=filename,
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            type="primary",
                            use_container_width=True
                        )
                        
                        # æ˜¾ç¤ºtokenä½¿ç”¨ç»Ÿè®¡
                        if VERBOSE:
                            with st.expander("ğŸ“Š å¤„ç†ç»Ÿè®¡", expanded=False):
                                token_usage = app.llm_service.get_token_usage()
                                embedding_usage = app.embedding_service.get_token_usage()
                                
                                col1, col2 = st.columns(2)
                                
                                with col1:
                                    st.subheader("LLMä½¿ç”¨ç»Ÿè®¡")
                                    st.write(f"æ€»Tokenæ•°: {token_usage['total_tokens']}")
                                    st.write(f"è¯·æ±‚æ¬¡æ•°: {token_usage['request_count']}")
                                    st.write(f"å¹³å‡Token/è¯·æ±‚: {token_usage['avg_tokens_per_request']}")
                                
                                with col2:
                                    st.subheader("Embeddingä½¿ç”¨ç»Ÿè®¡")
                                    st.write(f"æ€»Tokenæ•°: {embedding_usage['total_tokens']}")
                                    st.write(f"è¯·æ±‚æ¬¡æ•°: {embedding_usage['request_count']}")
                                    st.write(f"å¹³å‡Token/è¯·æ±‚: {embedding_usage['avg_tokens_per_request']}")
                        
                        st.success("ğŸ‰ åŒ¹é…å®Œæˆï¼æ‚¨å¯ä»¥ä¸‹è½½ç»“æœæ–‡ä»¶ã€‚")
                        
                    except Exception as e:
                        st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
                        import traceback
                        if VERBOSE:
                            st.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        
        except Exception as e:
            st.error(f"è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            st.info("è¯·ç¡®ä¿æ–‡ä»¶æ˜¯UTF-8 BOMç¼–ç çš„CSVæ ¼å¼")
    
    else:
        st.info("ğŸ‘† è¯·ä¸Šä¼ CSVæ–‡ä»¶å¼€å§‹åŒ¹é…")
        
        # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
        with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜", expanded=True):
            st.markdown("""
            ### ğŸ“‹ ä½¿ç”¨æ­¥éª¤
            
            1. **å‡†å¤‡æ–‡ä»¶**: ç¡®ä¿æ‚¨çš„CSVæ–‡ä»¶ä½¿ç”¨UTF-8 BOMç¼–ç 
            2. **ä¸Šä¼ æ–‡ä»¶**: ç‚¹å‡»ä¸Šæ–¹çš„æ–‡ä»¶ä¸Šä¼ åŒºåŸŸé€‰æ‹©æ‚¨çš„CSVæ–‡ä»¶
            3. **æ£€æŸ¥é¢„è§ˆ**: ç¡®è®¤ç¬¬ä¸€åˆ—åŒ…å«éœ€è¦åŒ¹é…çš„é¢˜ç›®æ–‡æœ¬
            4. **å¼€å§‹åŒ¹é…**: ç‚¹å‡»"å¼€å§‹åŒ¹é…"æŒ‰é’®
            5. **ä¸‹è½½ç»“æœ**: å¤„ç†å®Œæˆåä¸‹è½½Excelæ ¼å¼çš„ç»“æœæ–‡ä»¶
            
            ### âš™ï¸ ç³»ç»Ÿç‰¹æ€§
             
             - **æ™ºèƒ½åŒ¹é…**: ç»“åˆå‘é‡ç›¸ä¼¼åº¦å’ŒLLMè¯­ä¹‰ç†è§£
             - **é«˜æ•ˆå¤„ç†**: æ”¯æŒæ‰¹é‡å¹¶å‘å¤„ç†
             - **æ°´å¹³æ’åˆ—**: æ¯è¡Œæ˜¾ç¤ºä¸€ä¸ªé¢˜ç›®åŠå…¶å‰3ä¸ªæœ€ç›¸ä¼¼é¢˜ç›®
             - **Excelå¯¼å‡º**: ç»“æœä»¥Excelæ ¼å¼æä¾›ä¸‹è½½
            
            ### ğŸ“Š è¾“å‡ºæ ¼å¼
             
             ç»“æœæ–‡ä»¶åŒ…å«ä»¥ä¸‹åˆ—ï¼š
             - **é¢˜ç›®**: æ‚¨ä¸Šä¼ çš„åŸå§‹é¢˜ç›®
             - **ç›¸ä¼¼é¢˜ç›®1**: æœ€ç›¸ä¼¼çš„é¢˜ç›®
             - **ç›¸ä¼¼åº¦1**: å¯¹åº”çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ä¹‹é—´ï¼‰
             - **ç›¸ä¼¼é¢˜ç›®2**: ç¬¬äºŒç›¸ä¼¼çš„é¢˜ç›®
             - **ç›¸ä¼¼åº¦2**: å¯¹åº”çš„ç›¸ä¼¼åº¦åˆ†æ•°
             - **ç›¸ä¼¼é¢˜ç›®3**: ç¬¬ä¸‰ç›¸ä¼¼çš„é¢˜ç›®
             - **ç›¸ä¼¼åº¦3**: å¯¹åº”çš„ç›¸ä¼¼åº¦åˆ†æ•°
            """)


if __name__ == "__main__":
    main()